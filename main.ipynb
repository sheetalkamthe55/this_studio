{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG: Data Ingestion and Retrieval for complex documents\n",
    "\n",
    "\n",
    "<img src=\"arch.png\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from rag_101.retriever import (\n",
    "    create_parent_retriever,\n",
    "    load_embedding_model,\n",
    "    load_pdf,\n",
    "    load_reranker_model,\n",
    "    retrieve_context,\n",
    ")\n",
    "from rich import print"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load PDF documents and create retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"/teamspace/studios/this_studio/example_data/2401.00908.pdf\",  # DocLLM paper\n",
    "]\n",
    "\n",
    "docs = load_pdf(files=files)\n",
    "\n",
    "embedding_model = load_embedding_model()\n",
    "retriever = create_parent_retriever(docs, embedding_model)\n",
    "reranker_model = load_reranker_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Langchain's Ollama Chat API integration using `mistral` model and create the chain with prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"mistral\")\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    (\n",
    "        \"Please answer the following question based on the provided `context` that follows the question.\\n\"\n",
    "        \"If you do not know the answer then just say 'I do not know'\\n\"\n",
    "        \"question: {question}\\n\"\n",
    "        \"context: ```{context}```\\n\"\n",
    "    )\n",
    ")\n",
    "chain = prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve document and run the chain using `context` and `question`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Response:  The model was trained on datasets from two primary sources: IIT-CDIP Test Collection <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span> and \n",
       "DocBank.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Response:  The model was trained on datasets from two primary sources: IIT-CDIP Test Collection \u001b[1;36m1.0\u001b[0m and \n",
       "DocBank.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is the source of the dataset the model was trained on?\"\n",
    "context, similarity_score = retrieve_context(query, retriever, reranker_model)[0]\n",
    "context = context.page_content\n",
    "\n",
    "output = chain.invoke({\"context\": context, \"question\": query})\n",
    "print(\"LLM Response:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through some sample queries\n",
    "samples = [\n",
    "    \"What are the metrics used to evaluate the answers?\",\n",
    "    \"How many pdf data were collected from the USA?\",\n",
    "    \"What is the DocLLM architecture ?\",\n",
    "    \"Which countries were used to collect dataset?\",\n",
    "    \"Where was the agriculture dataset collected for the USA?\",\n",
    "    \"how was the content and structure of available documents augmented?\",\n",
    "    \"What was the answer generation process used in the paper?\",\n",
    "    \"how was the content and structure of available documents augmented?\",\n",
    "    \"Explain the DocLLM model architecture step by step\",\n",
    "    \"what is the training dataset used in DocLLM?\",\n",
    "    \"which pretrained model was used for DocLLM?\",\n",
    "    \"which tools were used for web scraping?\",\n",
    "    \"which tools were used to extract information from pdfs?\",\n",
    "    \"what is GROBID mentioned in the paper?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What are the metrics used to evaluate the answers?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "What are the metrics used to evaluate the answers?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Response:  The metrics used to evaluate the answers in this context include Average Normalized Levenshtein \n",
       "Similarity <span style=\"font-weight: bold\">(</span>ANLS<span style=\"font-weight: bold\">)</span> for most VQA datasets, CIDEr for VisualMRC, accuracy for Performance on all CLS and NLI datasets,\n",
       "F1 score for all KIE datasets, and accuracy for classification performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Response:  The metrics used to evaluate the answers in this context include Average Normalized Levenshtein \n",
       "Similarity \u001b[1m(\u001b[0mANLS\u001b[1m)\u001b[0m for most VQA datasets, CIDEr for VisualMRC, accuracy for Performance on all CLS and NLI datasets,\n",
       "F1 score for all KIE datasets, and accuracy for classification performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       " ==================================================================================================== \n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       " ==================================================================================================== \n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">How many pdf data were collected from the USA?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "How many pdf data were collected from the USA?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Response:  I do not know\n",
       "\n",
       "The context provided does not mention the number of PDF data collected specifically from the USA. The context only \n",
       "mentions various datasets and their sources, which include conferences held in different countries such as \n",
       "Ethiopia, Switzerland, and Australia.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Response:  I do not know\n",
       "\n",
       "The context provided does not mention the number of PDF data collected specifically from the USA. The context only \n",
       "mentions various datasets and their sources, which include conferences held in different countries such as \n",
       "Ethiopia, Switzerland, and Australia.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       " ==================================================================================================== \n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       " ==================================================================================================== \n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What is the DocLLM architecture ?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "What is the DocLLM architecture ?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Response:  The DocLLM architecture is a multi-modal model for document intelligence tasks that incorporates \n",
       "spatial layout information intrinsically, without relying on a complex vision encoder. It does this by extending \n",
       "self-attention mechanisms to include new attention scores that capture cross-modal relationships between text and \n",
       "spatial layouts using bounding box coordinates obtained from OCR. This approach preserves the causal decoder \n",
       "architecture, introduces only a marginal increase in model size, and reduces processing times. It has been \n",
       "demonstrated to be effective for various document intelligence tasks such as form understanding, table alignment, \n",
       "and visual question answering.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Response:  The DocLLM architecture is a multi-modal model for document intelligence tasks that incorporates \n",
       "spatial layout information intrinsically, without relying on a complex vision encoder. It does this by extending \n",
       "self-attention mechanisms to include new attention scores that capture cross-modal relationships between text and \n",
       "spatial layouts using bounding box coordinates obtained from OCR. This approach preserves the causal decoder \n",
       "architecture, introduces only a marginal increase in model size, and reduces processing times. It has been \n",
       "demonstrated to be effective for various document intelligence tasks such as form understanding, table alignment, \n",
       "and visual question answering.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       " ==================================================================================================== \n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       " ==================================================================================================== \n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Which countries were used to collect dataset?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Which countries were used to collect dataset?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Response:  The context mentions several datasets used in different research papers, but it does not explicitly \n",
       "state which countries were used to collect the data for any of these datasets. Therefore, I do not know the answer \n",
       "to that question based on the given context.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Response:  The context mentions several datasets used in different research papers, but it does not explicitly \n",
       "state which countries were used to collect the data for any of these datasets. Therefore, I do not know the answer \n",
       "to that question based on the given context.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       " ==================================================================================================== \n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       " ==================================================================================================== \n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Where was the agriculture dataset collected for the USA?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Where was the agriculture dataset collected for the USA?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Response:  I do not know\n",
       "\n",
       "The context provided does not contain any information about where the agriculture dataset for the USA was \n",
       "collected.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Response:  I do not know\n",
       "\n",
       "The context provided does not contain any information about where the agriculture dataset for the USA was \n",
       "collected.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       " ==================================================================================================== \n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       " ==================================================================================================== \n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">how was the content and structure of available documents augmented?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "how was the content and structure of available documents augmented?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Response:  The content and structure of available documents were augmented by adopting cohesive blocks of text \n",
       "for broader contexts during the self-supervised pre-training phase, and implementing an infilling approach that \n",
       "conditions the prediction on both preceding and succeeding tokens. These modifications enable the model to better \n",
       "handle misaligned text, contextual completions, intricate layouts, and mixed data types in visual documents. The \n",
       "pre-trained knowledge of DocLLM was then fine-tuned on instruction data curated from several datasets for various \n",
       "document intelligence tasks such as key information extraction, natural language inference, visual \n",
       "question-answering, and document classification. Layout hints like field separators, titles, and captions were \n",
       "integrated during instruction-tuning to facilitate learning the logical structure of documents. The performance \n",
       "improvement ranging from <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>% to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">61</span>% was observed for the Llama2-7B model in four out of five previously unseen \n",
       "datasets.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Response:  The content and structure of available documents were augmented by adopting cohesive blocks of text \n",
       "for broader contexts during the self-supervised pre-training phase, and implementing an infilling approach that \n",
       "conditions the prediction on both preceding and succeeding tokens. These modifications enable the model to better \n",
       "handle misaligned text, contextual completions, intricate layouts, and mixed data types in visual documents. The \n",
       "pre-trained knowledge of DocLLM was then fine-tuned on instruction data curated from several datasets for various \n",
       "document intelligence tasks such as key information extraction, natural language inference, visual \n",
       "question-answering, and document classification. Layout hints like field separators, titles, and captions were \n",
       "integrated during instruction-tuning to facilitate learning the logical structure of documents. The performance \n",
       "improvement ranging from \u001b[1;36m15\u001b[0m% to \u001b[1;36m61\u001b[0m% was observed for the Llama2-7B model in four out of five previously unseen \n",
       "datasets.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       " ==================================================================================================== \n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       " ==================================================================================================== \n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What was the answer generation process used in the paper?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "What was the answer generation process used in the paper?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Response:  Based on the provided context, none of the mentioned papers explicitly describe the specific answer \n",
       "generation process used in their research. The context focuses mostly on the titles and authors of various papers \n",
       "related to language models and filling-in tasks.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Response:  Based on the provided context, none of the mentioned papers explicitly describe the specific answer \n",
       "generation process used in their research. The context focuses mostly on the titles and authors of various papers \n",
       "related to language models and filling-in tasks.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       " ==================================================================================================== \n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       " ==================================================================================================== \n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">how was the content and structure of available documents augmented?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "how was the content and structure of available documents augmented?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Response:  The content and structure of available documents were augmented through two modifications during the\n",
       "self-supervised pre-training phase of a language model <span style=\"font-weight: bold\">(</span>DocLLM<span style=\"font-weight: bold\">)</span>. The first modification was adopting cohesive \n",
       "blocks of text that account for broader contexts instead of focusing on individual next tokens. The second \n",
       "modification was implementing an infilling approach, which conditions the prediction on both preceding and \n",
       "succeeding tokens. These modifications enabled the model to better handle misaligned text, contextual completions, \n",
       "intricate layouts, and mixed data types in visual documents. The pre-trained knowledge of DocLLM was then \n",
       "fine-tuned for various document intelligence tasks using instruction data from multiple datasets, including key \n",
       "information extraction, natural language inference, visual question-answering, and document classification. Layout \n",
       "hints such as field separators, titles, and captions were integrated during instruction-tuning to facilitate \n",
       "learning the logical structure of the documents. The performance improvement for the Llama2-7B model ranged from \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>% to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">61</span>% in four out of five previously unseen datasets.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Response:  The content and structure of available documents were augmented through two modifications during the\n",
       "self-supervised pre-training phase of a language model \u001b[1m(\u001b[0mDocLLM\u001b[1m)\u001b[0m. The first modification was adopting cohesive \n",
       "blocks of text that account for broader contexts instead of focusing on individual next tokens. The second \n",
       "modification was implementing an infilling approach, which conditions the prediction on both preceding and \n",
       "succeeding tokens. These modifications enabled the model to better handle misaligned text, contextual completions, \n",
       "intricate layouts, and mixed data types in visual documents. The pre-trained knowledge of DocLLM was then \n",
       "fine-tuned for various document intelligence tasks using instruction data from multiple datasets, including key \n",
       "information extraction, natural language inference, visual question-answering, and document classification. Layout \n",
       "hints such as field separators, titles, and captions were integrated during instruction-tuning to facilitate \n",
       "learning the logical structure of the documents. The performance improvement for the Llama2-7B model ranged from \n",
       "\u001b[1;36m15\u001b[0m% to \u001b[1;36m61\u001b[0m% in four out of five previously unseen datasets.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       " ==================================================================================================== \n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       " ==================================================================================================== \n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Explain the DocLLM model architecture step by step\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Explain the DocLLM model architecture step by step\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Response:  The DocLLM model is a multi-modal model designed to understand document intelligence tasks by \n",
       "incorporating spatial layout information. Here's a step-by-step explanation of its architecture based on the \n",
       "provided context:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Input documents consist of text tokens and their bounding boxes obtained using optical character recognition \n",
       "<span style=\"font-weight: bold\">(</span>OCR<span style=\"font-weight: bold\">)</span>. The spatial layout information is incorporated through these bounding box coordinates, without relying on \n",
       "any vision encoder component.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. The attention mechanism of Language Models <span style=\"font-weight: bold\">(</span>LLMs<span style=\"font-weight: bold\">)</span> is extended to capture dependencies between text semantics and\n",
       "spatial layouts. This is done by introducing new attention scores that capture cross-modal relationships between \n",
       "the text and spatial modalities.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. The self-attention mechanism of standard transformers is extended to include these new attention scores, \n",
       "allowing for the representation of alignments between content, position, and size of fields at various abstraction \n",
       "levels across the transformer layers. This enhancement aims to improve document understanding.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Infilling text blocks are used as pre-training objectives.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. Task adaptation is performed on a newly collated dataset of instructions. The model's performance on tasks such \n",
       "as form understanding, table alignment, and visual question answering is demonstrated to be sufficient by merely \n",
       "including the spatial layout structure.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Response:  The DocLLM model is a multi-modal model designed to understand document intelligence tasks by \n",
       "incorporating spatial layout information. Here's a step-by-step explanation of its architecture based on the \n",
       "provided context:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. Input documents consist of text tokens and their bounding boxes obtained using optical character recognition \n",
       "\u001b[1m(\u001b[0mOCR\u001b[1m)\u001b[0m. The spatial layout information is incorporated through these bounding box coordinates, without relying on \n",
       "any vision encoder component.\n",
       "\u001b[1;36m2\u001b[0m. The attention mechanism of Language Models \u001b[1m(\u001b[0mLLMs\u001b[1m)\u001b[0m is extended to capture dependencies between text semantics and\n",
       "spatial layouts. This is done by introducing new attention scores that capture cross-modal relationships between \n",
       "the text and spatial modalities.\n",
       "\u001b[1;36m3\u001b[0m. The self-attention mechanism of standard transformers is extended to include these new attention scores, \n",
       "allowing for the representation of alignments between content, position, and size of fields at various abstraction \n",
       "levels across the transformer layers. This enhancement aims to improve document understanding.\n",
       "\u001b[1;36m4\u001b[0m. Infilling text blocks are used as pre-training objectives.\n",
       "\u001b[1;36m5\u001b[0m. Task adaptation is performed on a newly collated dataset of instructions. The model's performance on tasks such \n",
       "as form understanding, table alignment, and visual question answering is demonstrated to be sufficient by merely \n",
       "including the spatial layout structure.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       " ==================================================================================================== \n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       " ==================================================================================================== \n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">what is the training dataset used in DocLLM?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "what is the training dataset used in DocLLM?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Response:  Based on the context provided, there is no clear mention of what dataset is used for training DocLLM\n",
       "model. The context only mentions that they use <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> out of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> datasets for instruction-tuning, and evaluate DocLLM on\n",
       "the test split of the remaining three datasets. Therefore, I do not know the answer to your question.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Response:  Based on the context provided, there is no clear mention of what dataset is used for training DocLLM\n",
       "model. The context only mentions that they use \u001b[1;36m11\u001b[0m out of \u001b[1;36m16\u001b[0m datasets for instruction-tuning, and evaluate DocLLM on\n",
       "the test split of the remaining three datasets. Therefore, I do not know the answer to your question.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       " ==================================================================================================== \n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       " ==================================================================================================== \n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">which pretrained model was used for DocLLM?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "which pretrained model was used for DocLLM?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Response:  I do not know which specific pretrained model was used for DocLLM based on the given context. The \n",
       "context mentions that DocLLM is built upon Falcon-1B and Llama2-7B architectures, but it does not explicitly state \n",
       "which pretrained models were used for these architectures.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Response:  I do not know which specific pretrained model was used for DocLLM based on the given context. The \n",
       "context mentions that DocLLM is built upon Falcon-1B and Llama2-7B architectures, but it does not explicitly state \n",
       "which pretrained models were used for these architectures.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       " ==================================================================================================== \n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       " ==================================================================================================== \n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">which tools were used for web scraping?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "which tools were used for web scraping?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Response:  The context provided does not mention any specific tools used for web scraping. The given papers \n",
       "focus on various machine learning models and techniques for image recognition, document understanding, form \n",
       "document information extraction, and universal document processing.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Response:  The context provided does not mention any specific tools used for web scraping. The given papers \n",
       "focus on various machine learning models and techniques for image recognition, document understanding, form \n",
       "document information extraction, and universal document processing.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       " ==================================================================================================== \n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       " ==================================================================================================== \n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">which tools were used to extract information from pdfs?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "which tools were used to extract information from pdfs?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Response:  I do not know which tools were used to extract information from PDFs based on the provided context. \n",
       "The context focuses on a machine learning model called DocLLM that is designed to understand and process \n",
       "information from visual documents, such as those found in PDF format. However, it does not mention any specific \n",
       "tools or techniques for extracting information from these documents before training the model.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Response:  I do not know which tools were used to extract information from PDFs based on the provided context. \n",
       "The context focuses on a machine learning model called DocLLM that is designed to understand and process \n",
       "information from visual documents, such as those found in PDF format. However, it does not mention any specific \n",
       "tools or techniques for extracting information from these documents before training the model.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       " ==================================================================================================== \n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       " ==================================================================================================== \n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">what is GROBID mentioned in the paper?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "what is GROBID mentioned in the paper?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Response:  In the context provided, GROBID is not mentioned in any of the given papers. However, one paper, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"LayoutLMv2: Multi-modal pre-training for visually-rich document understanding\"</span> by Xu et al., mentions the use of \n",
       "layout analysis in their work, but they do not specifically mention GROBID. If you have more context or information\n",
       "about a specific implementation or tool used in these papers that goes by the name GROBID, then I might be able to \n",
       "help with that. Otherwise, I do not know if or how GROBID is mentioned in these papers.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Response:  In the context provided, GROBID is not mentioned in any of the given papers. However, one paper, \n",
       "\u001b[32m\"LayoutLMv2: Multi-modal pre-training for visually-rich document understanding\"\u001b[0m by Xu et al., mentions the use of \n",
       "layout analysis in their work, but they do not specifically mention GROBID. If you have more context or information\n",
       "about a specific implementation or tool used in these papers that goes by the name GROBID, then I might be able to \n",
       "help with that. Otherwise, I do not know if or how GROBID is mentioned in these papers.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       " ==================================================================================================== \n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       " ==================================================================================================== \n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for query in samples:\n",
    "    print(query)\n",
    "    context = retrieve_context(\n",
    "        query, retriever=retriever, reranker_model=reranker_model\n",
    "    )[0]\n",
    "    output = chain.invoke({\"context\": context[0].page_content, \"question\": query})\n",
    "    print(\"LLM Response:\", output)\n",
    "\n",
    "    print(\"\\n\", \"=\" * 100, \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
